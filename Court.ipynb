{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\se99a\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\se99a\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\se99a\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tesnorflow (from versions: none)\n",
      "ERROR: No matching distribution found for tesnorflow\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install tesnorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\se99a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          first_party                    second_party  \\\n",
       "0   Phil A. St. Amant              Herman A. Thompson   \n",
       "1      Stephen Duncan                  Lawrence Owens   \n",
       "2   Billy Joe Magwood  Tony Patterson, Warden, et al.   \n",
       "3          Linkletter                          Walker   \n",
       "4  William Earl Fikes                         Alabama   \n",
       "\n",
       "                                               facts  first_party_winner  \n",
       "0  On June 27, 1962, Phil St. Amant, a candidate ...                   1  \n",
       "1  Ramon Nelson was riding his bike when he suffe...                   0  \n",
       "2  An Alabama state court convicted Billy Joe Mag...                   1  \n",
       "3  Victor Linkletter was convicted in state court...                   0  \n",
       "4  On April 24, 1953 in Selma, Alabama, an intrud...                   1  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.drop('ID',axis =1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2478 entries, 0 to 2477\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   first_party         2478 non-null   object\n",
      " 1   second_party        2478 non-null   object\n",
      " 2   facts               2478 non-null   object\n",
      " 3   first_party_winner  2478 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 77.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2110"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['first_party'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGYCAYAAACzlLNPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlmUlEQVR4nO3df1DUd37H8dceC0QZ+EYg7Gbn1ujNUE8PmuRIihB76oBoKnKe02JKStMpNWZMsESIkbHpeZk5SGxPbI+eMakNnsbj/uhh0yZHxPbOhMEfiEfutCZpesTAyYZcS74rhi4cbv/I5Du3EE0wi/CB52PmO+N+v+/vzmczt8dzvnx3cYXD4bAAAAAM84XJXgAAAMD1IGIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGMk92QuYKFeuXNHFixeVmJgol8s12csBAACfQTgc1qVLl+Tz+fSFL1z7Wsu0jZiLFy/K7/dP9jIAAMB16O7u1he/+MVrzkzbiElMTJT00X+EpKSkSV4NAAD4LILBoPx+v/Nz/FqmbcR8/CukpKQkIgYAAMN8lltBuLEXAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGck/2AhB987a9NNlLwA30zlOrJ3sJADApuBIDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACONO2JeffVVrVmzRj6fTy6XS4cPHx4zc/78eRUVFcmyLCUmJmrx4sV69913neOhUEjl5eVKTU1VQkKCioqK1NPTE/Ec/f39Ki0tlWVZsixLpaWl+uCDD8b9AgEAwPQ07oi5fPmybr/9dtXX13/i8f/+7//WkiVL9OUvf1k//elP9frrr+uJJ57QTTfd5MxUVFSoqalJjY2Nam1t1cDAgAoLCzUyMuLMlJSUqLOzU83NzWpublZnZ6dKS0uv4yUCAIDpyBUOh8PXfbLLpaamJq1du9bZd9999yk2NlYHDhz4xHNs29Ytt9yiAwcOaP369ZKkixcvyu/36+WXX9bKlSt1/vx5LVq0SCdOnFB2drYk6cSJE8rJydEbb7yhBQsWfOragsGgLMuSbdtKSkq63pdoJL7sbmbhy+4ATCfj+fkd1Xtirly5opdeekm/8zu/o5UrVyotLU3Z2dkRv3Lq6OjQ8PCwCgoKnH0+n08ZGRlqa2uTJB0/flyWZTkBI0mLFy+WZVnOzGihUEjBYDBiAwAA01dUI6avr08DAwN66qmntGrVKh05ckTf+MY3tG7dOh07dkySFAgEFBcXpzlz5kSc6/F4FAgEnJm0tLQxz5+WlubMjFZbW+vcP2NZlvx+fzRfGgAAmGKifiVGkr7+9a/r0Ucf1R133KFt27apsLBQzzzzzDXPDYfDcrlczuPf/vfVZn5bdXW1bNt2tu7u7s/xSgAAwFQX1YhJTU2V2+3WokWLIvYvXLjQ+XSS1+vV0NCQ+vv7I2b6+vrk8Xicmffee2/M87///vvOzGjx8fFKSkqK2AAAwPQV1YiJi4vT3XffrTfffDNi/1tvvaXbbrtNkpSVlaXY2Fi1tLQ4x3t7e3X27Fnl5uZKknJycmTbtk6dOuXMnDx5UrZtOzMAAGBmc4/3hIGBAb399tvO466uLnV2dio5OVlz587VY489pvXr1+trX/uali9frubmZv3rv/6rfvrTn0qSLMtSWVmZKisrlZKSouTkZFVVVSkzM1P5+fmSPrpys2rVKm3YsEF79+6VJD344IMqLCz8TJ9MAgAA09+4I+b06dNavny583jLli2SpAceeEANDQ36xje+oWeeeUa1tbXavHmzFixYoH/+53/WkiVLnHPq6urkdrtVXFyswcFB5eXlqaGhQTExMc7MCy+8oM2bNzufYioqKrrqd9MAAICZ53N9T8xUxvfEYKbge2IATCeT9j0xAAAANwoRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjjTtiXn31Va1Zs0Y+n08ul0uHDx++6uzGjRvlcrm0e/fuiP2hUEjl5eVKTU1VQkKCioqK1NPTEzHT39+v0tJSWZYly7JUWlqqDz74YLzLBQAA09S4I+by5cu6/fbbVV9ff825w4cP6+TJk/L5fGOOVVRUqKmpSY2NjWptbdXAwIAKCws1MjLizJSUlKizs1PNzc1qbm5WZ2enSktLx7tcAAAwTbnHe8K9996re++995ozv/rVr/TII4/olVde0erVqyOO2batffv26cCBA8rPz5ckHTx4UH6/X0ePHtXKlSt1/vx5NTc368SJE8rOzpYkPffcc8rJydGbb76pBQsWjHfZAABgmon6PTFXrlxRaWmpHnvsMX3lK18Zc7yjo0PDw8MqKChw9vl8PmVkZKitrU2SdPz4cVmW5QSMJC1evFiWZTkzAABgZhv3lZhP8/TTT8vtdmvz5s2feDwQCCguLk5z5syJ2O/xeBQIBJyZtLS0MeempaU5M6OFQiGFQiHncTAYvN6XAAAADBDVKzEdHR36u7/7OzU0NMjlco3r3HA4HHHOJ50/eua31dbWOjcBW5Ylv98/vsUDAACjRDViXnvtNfX19Wnu3Llyu91yu926cOGCKisrNW/ePEmS1+vV0NCQ+vv7I87t6+uTx+NxZt57770xz//+++87M6NVV1fLtm1n6+7ujuZLAwAAU0xUI6a0tFQ///nP1dnZ6Ww+n0+PPfaYXnnlFUlSVlaWYmNj1dLS4pzX29urs2fPKjc3V5KUk5Mj27Z16tQpZ+bkyZOybduZGS0+Pl5JSUkRGwAAmL7GfU/MwMCA3n77bedxV1eXOjs7lZycrLlz5yolJSViPjY2Vl6v1/lEkWVZKisrU2VlpVJSUpScnKyqqiplZmY6n1ZauHChVq1apQ0bNmjv3r2SpAcffFCFhYV8MgkAAEi6jog5ffq0li9f7jzesmWLJOmBBx5QQ0PDZ3qOuro6ud1uFRcXa3BwUHl5eWpoaFBMTIwz88ILL2jz5s3Op5iKioo+9btpAADAzOEKh8PhyV7ERAgGg7IsS7Ztz7hfLc3b9tJkLwE30DtPrf70IQAwxHh+fvO3kwAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGGnfEvPrqq1qzZo18Pp9cLpcOHz7sHBseHtbjjz+uzMxMJSQkyOfz6U//9E918eLFiOcIhUIqLy9XamqqEhISVFRUpJ6enoiZ/v5+lZaWyrIsWZal0tJSffDBB9f1IgEAwPQz7oi5fPmybr/9dtXX14859uGHH+rMmTN64okndObMGf3oRz/SW2+9paKiooi5iooKNTU1qbGxUa2trRoYGFBhYaFGRkacmZKSEnV2dqq5uVnNzc3q7OxUaWnpdbxEAAAwHbnC4XD4uk92udTU1KS1a9dedaa9vV2/93u/pwsXLmju3LmybVu33HKLDhw4oPXr10uSLl68KL/fr5dfflkrV67U+fPntWjRIp04cULZ2dmSpBMnTignJ0dvvPGGFixY8KlrCwaDsixLtm0rKSnpel+ikeZte2myl4Ab6J2nVk/2EgAgasbz83vC74mxbVsul0s333yzJKmjo0PDw8MqKChwZnw+nzIyMtTW1iZJOn78uCzLcgJGkhYvXizLspyZ0UKhkILBYMQGAACmrwmNmP/7v//Ttm3bVFJS4tRUIBBQXFyc5syZEzHr8XgUCAScmbS0tDHPl5aW5syMVltb69w/Y1mW/H5/lF8NAACYSiYsYoaHh3XffffpypUr+t73vvep8+FwWC6Xy3n82/++2sxvq66ulm3bztbd3X39iwcAAFPehETM8PCwiouL1dXVpZaWlojfaXm9Xg0NDam/vz/inL6+Pnk8HmfmvffeG/O877//vjMzWnx8vJKSkiI2AAAwfUU9Yj4OmP/6r//S0aNHlZKSEnE8KytLsbGxamlpcfb19vbq7Nmzys3NlSTl5OTItm2dOnXKmTl58qRs23ZmAADAzOYe7wkDAwN6++23ncddXV3q7OxUcnKyfD6f/vAP/1BnzpzRv/3bv2lkZMS5hyU5OVlxcXGyLEtlZWWqrKxUSkqKkpOTVVVVpczMTOXn50uSFi5cqFWrVmnDhg3au3evJOnBBx9UYWHhZ/pkEgAAmP7GHTGnT5/W8uXLncdbtmyRJD3wwAPasWOHXnzxRUnSHXfcEXHeT37yEy1btkySVFdXJ7fbreLiYg0ODiovL08NDQ2KiYlx5l944QVt3rzZ+RRTUVHRJ343DQAAmJk+1/fETGV8TwxmCr4nBsB0MqW+JwYAAGAiEDEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjDTuiHn11Ve1Zs0a+Xw+uVwuHT58OOJ4OBzWjh075PP5NGvWLC1btkznzp2LmAmFQiovL1dqaqoSEhJUVFSknp6eiJn+/n6VlpbKsixZlqXS0lJ98MEH436BAABgehp3xFy+fFm333676uvrP/H4zp07tWvXLtXX16u9vV1er1crVqzQpUuXnJmKigo1NTWpsbFRra2tGhgYUGFhoUZGRpyZkpISdXZ2qrm5Wc3Nzers7FRpael1vEQAADAducLhcPi6T3a51NTUpLVr10r66CqMz+dTRUWFHn/8cUkfXXXxeDx6+umntXHjRtm2rVtuuUUHDhzQ+vXrJUkXL16U3+/Xyy+/rJUrV+r8+fNatGiRTpw4oezsbEnSiRMnlJOTozfeeEMLFiz41LUFg0FZliXbtpWUlHS9L9FI87a9NNlLwA30zlOrJ3sJABA14/n5HdV7Yrq6uhQIBFRQUODsi4+P19KlS9XW1iZJ6ujo0PDwcMSMz+dTRkaGM3P8+HFZluUEjCQtXrxYlmU5M6OFQiEFg8GIDQAATF9RjZhAICBJ8ng8Efs9Ho9zLBAIKC4uTnPmzLnmTFpa2pjnT0tLc2ZGq62tde6fsSxLfr//c78eAAAwdU3Ip5NcLlfE43A4PGbfaKNnPmn+Ws9TXV0t27adrbu7+zpWDgAATBHViPF6vZI05mpJX1+fc3XG6/VqaGhI/f3915x57733xjz/+++/P+Yqz8fi4+OVlJQUsQEAgOkrqhEzf/58eb1etbS0OPuGhoZ07Ngx5ebmSpKysrIUGxsbMdPb26uzZ886Mzk5ObJtW6dOnXJmTp48Kdu2nRkAADCzucd7wsDAgN5++23ncVdXlzo7O5WcnKy5c+eqoqJCNTU1Sk9PV3p6umpqajR79myVlJRIkizLUllZmSorK5WSkqLk5GRVVVUpMzNT+fn5kqSFCxdq1apV2rBhg/bu3StJevDBB1VYWPiZPpkEAACmv3FHzOnTp7V8+XLn8ZYtWyRJDzzwgBoaGrR161YNDg5q06ZN6u/vV3Z2to4cOaLExETnnLq6OrndbhUXF2twcFB5eXlqaGhQTEyMM/PCCy9o8+bNzqeYioqKrvrdNAAAYOb5XN8TM5XxPTGYKfieGADTyaR9TwwAAMCNQsQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIzknuwFAAA+u3nbXprsJeAGeuep1ZO9hCmNKzEAAMBIUY+Y3/zmN/qrv/orzZ8/X7NmzdKXvvQlPfnkk7py5YozEw6HtWPHDvl8Ps2aNUvLli3TuXPnIp4nFAqpvLxcqampSkhIUFFRkXp6eqK9XAAAYKioR8zTTz+tZ555RvX19Tp//rx27typv/mbv9F3v/tdZ2bnzp3atWuX6uvr1d7eLq/XqxUrVujSpUvOTEVFhZqamtTY2KjW1lYNDAyosLBQIyMj0V4yAAAwUNTviTl+/Li+/vWva/Xqj36PN2/ePP3gBz/Q6dOnJX10FWb37t3avn271q1bJ0nav3+/PB6PDh06pI0bN8q2be3bt08HDhxQfn6+JOngwYPy+/06evSoVq5cGe1lAwAAw0T9SsySJUv07//+73rrrbckSa+//rpaW1v1B3/wB5Kkrq4uBQIBFRQUOOfEx8dr6dKlamtrkyR1dHRoeHg4Ysbn8ykjI8OZAQAAM1vUr8Q8/vjjsm1bX/7ylxUTE6ORkRF9+9vf1h//8R9LkgKBgCTJ4/FEnOfxeHThwgVnJi4uTnPmzBkz8/H5o4VCIYVCIedxMBiM2msCAABTT9SvxPzwhz/UwYMHdejQIZ05c0b79+/X3/7t32r//v0Rcy6XK+JxOBwes2+0a83U1tbKsixn8/v9n++FAACAKS3qEfPYY49p27Ztuu+++5SZmanS0lI9+uijqq2tlSR5vV5JGnNFpa+vz7k64/V6NTQ0pP7+/qvOjFZdXS3btp2tu7s72i8NAABMIVGPmA8//FBf+ELk08bExDgfsZ4/f768Xq9aWlqc40NDQzp27Jhyc3MlSVlZWYqNjY2Y6e3t1dmzZ52Z0eLj45WUlBSxAQCA6Svq98SsWbNG3/72tzV37lx95Stf0c9+9jPt2rVLf/7nfy7po18jVVRUqKamRunp6UpPT1dNTY1mz56tkpISSZJlWSorK1NlZaVSUlKUnJysqqoqZWZmOp9WAgAAM1vUI+a73/2unnjiCW3atEl9fX3y+XzauHGj/vqv/9qZ2bp1qwYHB7Vp0yb19/crOztbR44cUWJiojNTV1cnt9ut4uJiDQ4OKi8vTw0NDYqJiYn2kgEAgIFc4XA4PNmLmAjBYFCWZcm27Rn3qyX+tsrMwt9WmVl4f88sM/H9PZ6f3/ztJAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRJiRifvWrX+lP/uRPlJKSotmzZ+uOO+5QR0eHczwcDmvHjh3y+XyaNWuWli1bpnPnzkU8RygUUnl5uVJTU5WQkKCioiL19PRMxHIBAICBoh4x/f39uueeexQbG6sf//jH+s///E995zvf0c033+zM7Ny5U7t27VJ9fb3a29vl9Xq1YsUKXbp0yZmpqKhQU1OTGhsb1draqoGBARUWFmpkZCTaSwYAAAZyR/sJn376afn9fj3//PPOvnnz5jn/DofD2r17t7Zv365169ZJkvbv3y+Px6NDhw5p48aNsm1b+/bt04EDB5Sfny9JOnjwoPx+v44ePaqVK1dGe9kAAMAwUb8S8+KLL+quu+7SH/3RHyktLU133nmnnnvuOed4V1eXAoGACgoKnH3x8fFaunSp2traJEkdHR0aHh6OmPH5fMrIyHBmRguFQgoGgxEbAACYvqIeMb/85S+1Z88epaen65VXXtFDDz2kzZs36/vf/74kKRAISJI8Hk/EeR6PxzkWCAQUFxenOXPmXHVmtNraWlmW5Wx+vz/aLw0AAEwhUY+YK1eu6Ktf/apqamp05513auPGjdqwYYP27NkTMedyuSIeh8PhMftGu9ZMdXW1bNt2tu7u7s/3QgAAwJQW9Yi59dZbtWjRooh9Cxcu1LvvvitJ8nq9kjTmikpfX59zdcbr9WpoaEj9/f1XnRktPj5eSUlJERsAAJi+oh4x99xzj958882IfW+99ZZuu+02SdL8+fPl9XrV0tLiHB8aGtKxY8eUm5srScrKylJsbGzETG9vr86ePevMAACAmS3qn0569NFHlZubq5qaGhUXF+vUqVN69tln9eyzz0r66NdIFRUVqqmpUXp6utLT01VTU6PZs2erpKREkmRZlsrKylRZWamUlBQlJyerqqpKmZmZzqeVAADAzBb1iLn77rvV1NSk6upqPfnkk5o/f752796t+++/35nZunWrBgcHtWnTJvX39ys7O1tHjhxRYmKiM1NXVye3263i4mINDg4qLy9PDQ0NiomJifaSAQCAgVzhcDg82YuYCMFgUJZlybbtGXd/zLxtL032EnADvfPU6sleAm4g3t8zy0x8f4/n5zd/OwkAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgpAmPmNraWrlcLlVUVDj7wuGwduzYIZ/Pp1mzZmnZsmU6d+5cxHmhUEjl5eVKTU1VQkKCioqK1NPTM9HLBQAAhpjQiGlvb9ezzz6r3/3d343Yv3PnTu3atUv19fVqb2+X1+vVihUrdOnSJWemoqJCTU1NamxsVGtrqwYGBlRYWKiRkZGJXDIAADDEhEXMwMCA7r//fj333HOaM2eOsz8cDmv37t3avn271q1bp4yMDO3fv18ffvihDh06JEmybVv79u3Td77zHeXn5+vOO+/UwYMH9Ytf/EJHjx6dqCUDAACDTFjEPPzww1q9erXy8/Mj9nd1dSkQCKigoMDZFx8fr6VLl6qtrU2S1NHRoeHh4YgZn8+njIwMZ2a0UCikYDAYsQEAgOnLPRFP2tjYqDNnzqi9vX3MsUAgIEnyeDwR+z0ejy5cuODMxMXFRVzB+Xjm4/NHq62t1be+9a1oLB8AABgg6ldiuru79Zd/+Zc6ePCgbrrppqvOuVyuiMfhcHjMvtGuNVNdXS3btp2tu7t7/IsHAADGiHrEdHR0qK+vT1lZWXK73XK73Tp27Jj+/u//Xm6327kCM/qKSl9fn3PM6/VqaGhI/f39V50ZLT4+XklJSREbAACYvqIeMXl5efrFL36hzs5OZ7vrrrt0//33q7OzU1/60pfk9XrV0tLinDM0NKRjx44pNzdXkpSVlaXY2NiImd7eXp09e9aZAQAAM1vU74lJTExURkZGxL6EhASlpKQ4+ysqKlRTU6P09HSlp6erpqZGs2fPVklJiSTJsiyVlZWpsrJSKSkpSk5OVlVVlTIzM8fcKAwAAGamCbmx99Ns3bpVg4OD2rRpk/r7+5Wdna0jR44oMTHRmamrq5Pb7VZxcbEGBweVl5enhoYGxcTETMaSAQDAFOMKh8PhyV7ERAgGg7IsS7Ztz7j7Y+Zte2myl4Ab6J2nVk/2EnAD8f6eWWbi+3s8P7/520kAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjBT1iKmtrdXdd9+txMREpaWlae3atXrzzTcjZsLhsHbs2CGfz6dZs2Zp2bJlOnfuXMRMKBRSeXm5UlNTlZCQoKKiIvX09ER7uQAAwFBRj5hjx47p4Ycf1okTJ9TS0qLf/OY3Kigo0OXLl52ZnTt3ateuXaqvr1d7e7u8Xq9WrFihS5cuOTMVFRVqampSY2OjWltbNTAwoMLCQo2MjER7yQAAwEDuaD9hc3NzxOPnn39eaWlp6ujo0Ne+9jWFw2Ht3r1b27dv17p16yRJ+/fvl8fj0aFDh7Rx40bZtq19+/bpwIEDys/PlyQdPHhQfr9fR48e1cqVK6O9bAAAYJgJvyfGtm1JUnJysiSpq6tLgUBABQUFzkx8fLyWLl2qtrY2SVJHR4eGh4cjZnw+nzIyMpwZAAAws0X9SsxvC4fD2rJli5YsWaKMjAxJUiAQkCR5PJ6IWY/HowsXLjgzcXFxmjNnzpiZj88fLRQKKRQKOY+DwWDUXgcAAJh6JvRKzCOPPKKf//zn+sEPfjDmmMvlingcDofH7BvtWjO1tbWyLMvZ/H7/9S8cAABMeRMWMeXl5XrxxRf1k5/8RF/84hed/V6vV5LGXFHp6+tzrs54vV4NDQ2pv7//qjOjVVdXy7ZtZ+vu7o7mywEAAFNM1CMmHA7rkUce0Y9+9CP9x3/8h+bPnx9xfP78+fJ6vWppaXH2DQ0N6dixY8rNzZUkZWVlKTY2NmKmt7dXZ8+edWZGi4+PV1JSUsQGAACmr6jfE/Pwww/r0KFD+pd/+RclJiY6V1wsy9KsWbPkcrlUUVGhmpoapaenKz09XTU1NZo9e7ZKSkqc2bKyMlVWViolJUXJycmqqqpSZmam82klAAAws0U9Yvbs2SNJWrZsWcT+559/Xn/2Z38mSdq6dasGBwe1adMm9ff3Kzs7W0eOHFFiYqIzX1dXJ7fbreLiYg0ODiovL08NDQ2KiYmJ9pIBAICBXOFwODzZi5gIwWBQlmXJtu0Z96uledtemuwl4AZ656nVk70E3EC8v2eWmfj+Hs/Pb/52EgAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIUz5ivve972n+/Pm66aablJWVpddee22ylwQAAKaAKR0xP/zhD1VRUaHt27frZz/7mX7/939f9957r959993JXhoAAJhkUzpidu3apbKyMv3FX/yFFi5cqN27d8vv92vPnj2TvTQAADDJ3JO9gKsZGhpSR0eHtm3bFrG/oKBAbW1tY+ZDoZBCoZDz2LZtSVIwGJzYhU5BV0IfTvYScAPNxP+Nz2S8v2eWmfj+/vg1h8PhT52dshHz61//WiMjI/J4PBH7PR6PAoHAmPna2lp961vfGrPf7/dP2BqBqcDaPdkrADBRZvL7+9KlS7Is65ozUzZiPuZyuSIeh8PhMfskqbq6Wlu2bHEeX7lyRf/7v/+rlJSUT5zH9BIMBuX3+9Xd3a2kpKTJXg6AKOL9PbOEw2FdunRJPp/vU2enbMSkpqYqJiZmzFWXvr6+MVdnJCk+Pl7x8fER+26++eaJXCKmoKSkJP5PDpimeH/PHJ92BeZjU/bG3ri4OGVlZamlpSVif0tLi3JzcydpVQAAYKqYsldiJGnLli0qLS3VXXfdpZycHD377LN699139dBDD0320gAAwCSb0hGzfv16/c///I+efPJJ9fb2KiMjQy+//LJuu+22yV4appj4+Hh985vfHPMrRQDm4/2Nq3GFP8tnmAAAAKaYKXtPDAAAwLUQMQAAwEhEDAAAMBIRAwAAjETEAAAAI03pj1gDAGaenp4e7dmzR21tbQoEAnK5XPJ4PMrNzdVDDz3E38SDg49YY1rq7u7WN7/5Tf3TP/3TZC8FwDi0trbq3nvvld/vV0FBgTwej8LhsPr6+tTS0qLu7m79+Mc/1j333DPZS8UUQMRgWnr99df11a9+VSMjI5O9FADjcPfdd2vJkiWqq6v7xOOPPvqoWltb1d7efoNXhqmIiIGRXnzxxWse/+Uvf6nKykoiBjDMrFmz1NnZqQULFnzi8TfeeEN33nmnBgcHb/DKMBVxTwyMtHbtWrlcLl2rwV0u1w1cEYBouPXWW9XW1nbViDl+/LhuvfXWG7wqTFVEDIx066236h/+4R+0du3aTzze2dmprKysG7soAJ9bVVWVHnroIXV0dGjFihXyeDxyuVwKBAJqaWnRP/7jP2r37t2TvUxMEUQMjJSVlaUzZ85cNWI+7SoNgKlp06ZNSklJUV1dnfbu3ev8SjgmJkZZWVn6/ve/r+Li4kleJaYK7omBkV577TVdvnxZq1at+sTjly9f1unTp7V06dIbvDIA0TI8PKxf//rXkqTU1FTFxsZO8oow1RAxAADASHxjLwAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBI/w8dqyLrF4SJzAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['first_party_winner'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2006,  2238,  2676,  1010,  3705,  1010,  6316,  2358,  1012,\n",
       "         25933,  3372,  1010,  1037,  4018,  2005,  2270,  2436,  1010,  2081,\n",
       "          1037,  2547,  4613,  1999, 15302, 12801,  1010,  5773,  1012,  2076,\n",
       "          2023,  4613,  1010,  2358,  1012, 25933,  3372,  5496,  2010,  2576,\n",
       "          7116,  1997,  2108,  1037,  4750,  1998,  1997,  2108,  2920,  1999,\n",
       "          4735,  3450,  2007,  1996,  2132,  1997,  1996,  2334,  2780,  7747,\n",
       "          2586,  1012,  2633,  1010,  2358,  1012, 25933,  3372, 20467, 11458,\n",
       "          5953,  1010,  2019,  2264, 15302, 12801,  4112,  6458,  1010,  1999,\n",
       "          1037,  5679,  2000,  2693,  2769,  2090,  1996,  2780,  7747,  2586,\n",
       "          1998,  2358,  1012, 25933,  3372,  1521,  1055,  2576,  7116,  1012,\n",
       "          5953,  5147, 12923,  2358,  1012, 25933,  3372,  2005, 27652,  1012,\n",
       "          5773,  1521,  1055,  2034,  4984,  2457,  1997,  9023, 11674,  1010,\n",
       "          3173,  2008,  5953,  2106,  2025,  2265,  2358,  1012, 25933,  3372,\n",
       "          6051,  2007,  1523, 28238,  1012,  1524,  5953,  2059, 12068,  2000,\n",
       "          1996,  4259,  2457,  1997,  5773,  1012,  2008,  2457,  2218,  2008,\n",
       "          1010,  2348,  2270,  4481,  2005, 21156,  2070,  1997,  2037,  2034,\n",
       "          7450,  3860,  2013, 27652,  1010,  2358,  1012, 25933,  3372,  5496,\n",
       "          5953,  1997,  1037,  4126,  2007, 14395, 27770,  1997,  3251,  1996,\n",
       "         12629,  2020,  2995,  1012,  2633,  1010,  2008,  2457,  2218,  2008,\n",
       "          1996,  2034,  7450, 18227,  4895,  2378,  4048, 16313,  2098,  1010,\n",
       "         15873,  5981,  1010,  2738,  2084,  2019,  2330,  2161,  2000,  5607,\n",
       "          2091,  1996,  2204,  2171,  1997,  3087,  2040,  6433,  2000,  2022,\n",
       "          1037,  2270,  7947,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "text = df['facts'][0]\n",
    "tokens = tokenizer.encode_plus(text, add_special_tokens=True, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 11458,  1037,  1012,  5953,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df['second_party'][0]\n",
    "tokens = tokenizer.encode_plus(text, add_special_tokens=True, max_length=512, padding = 'do_not_pad' , truncation=True, return_tensors='pt')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 6316, 1037, 1012, 2358, 1012, 25933, 337...</td>\n",
       "      <td>[101, 11458, 1037, 1012, 5953, 102]</td>\n",
       "      <td>[101, 2006, 2238, 2676, 1010, 3705, 1010, 6316...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 4459, 7343, 102]</td>\n",
       "      <td>[101, 5623, 14824, 102]</td>\n",
       "      <td>[101, 12716, 5912, 2001, 5559, 2010, 7997, 204...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 5006, 3533, 23848, 3702, 102]</td>\n",
       "      <td>[101, 4116, 12424, 1010, 13745, 1010, 3802, 26...</td>\n",
       "      <td>[101, 2019, 6041, 2110, 2457, 7979, 5006, 3533...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 4957, 27901, 2099, 102]</td>\n",
       "      <td>[101, 5232, 102]</td>\n",
       "      <td>[101, 5125, 4957, 27901, 2099, 2001, 7979, 199...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 2520, 4656, 10882, 9681, 102]</td>\n",
       "      <td>[101, 6041, 102]</td>\n",
       "      <td>[101, 2006, 2258, 2484, 1010, 4052, 1999, 2811...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>[101, 9079, 12792, 3771, 17778, 28596, 1010, 1...</td>\n",
       "      <td>[101, 13918, 20145, 2523, 1010, 3802, 2632, 10...</td>\n",
       "      <td>[101, 3519, 13266, 1996, 4550, 2250, 2552, 208...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>[101, 26678, 4916, 2080, 2139, 4078, 2906, 284...</td>\n",
       "      <td>[101, 4707, 5416, 4636, 1010, 4297, 1012, 102]</td>\n",
       "      <td>[101, 4707, 5416, 4636, 1010, 4297, 1012, 1010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>[101, 25039, 13094, 2080, 102]</td>\n",
       "      <td>[101, 2142, 2163, 102]</td>\n",
       "      <td>[101, 1999, 2826, 1010, 1996, 2212, 2457, 7331...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>[101, 7521, 1998, 3019, 3989, 2326, 102]</td>\n",
       "      <td>[101, 2358, 1012, 22330, 2099, 102]</td>\n",
       "      <td>[101, 2006, 2233, 1022, 1010, 2727, 1010, 2198...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>[101, 2928, 2386, 102]</td>\n",
       "      <td>[101, 2225, 8584, 5693, 1010, 4297, 1012, 102]</td>\n",
       "      <td>[101, 7253, 2928, 2386, 8617, 1996, 7353, 2000...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2478 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            first_party  \\\n",
       "0     [101, 6316, 1037, 1012, 2358, 1012, 25933, 337...   \n",
       "1                                [101, 4459, 7343, 102]   \n",
       "2                   [101, 5006, 3533, 23848, 3702, 102]   \n",
       "3                         [101, 4957, 27901, 2099, 102]   \n",
       "4                   [101, 2520, 4656, 10882, 9681, 102]   \n",
       "...                                                 ...   \n",
       "2473  [101, 9079, 12792, 3771, 17778, 28596, 1010, 1...   \n",
       "2474  [101, 26678, 4916, 2080, 2139, 4078, 2906, 284...   \n",
       "2475                     [101, 25039, 13094, 2080, 102]   \n",
       "2476           [101, 7521, 1998, 3019, 3989, 2326, 102]   \n",
       "2477                             [101, 2928, 2386, 102]   \n",
       "\n",
       "                                           second_party  \\\n",
       "0                   [101, 11458, 1037, 1012, 5953, 102]   \n",
       "1                               [101, 5623, 14824, 102]   \n",
       "2     [101, 4116, 12424, 1010, 13745, 1010, 3802, 26...   \n",
       "3                                      [101, 5232, 102]   \n",
       "4                                      [101, 6041, 102]   \n",
       "...                                                 ...   \n",
       "2473  [101, 13918, 20145, 2523, 1010, 3802, 2632, 10...   \n",
       "2474     [101, 4707, 5416, 4636, 1010, 4297, 1012, 102]   \n",
       "2475                             [101, 2142, 2163, 102]   \n",
       "2476                [101, 2358, 1012, 22330, 2099, 102]   \n",
       "2477     [101, 2225, 8584, 5693, 1010, 4297, 1012, 102]   \n",
       "\n",
       "                                                  facts  first_party_winner  \n",
       "0     [101, 2006, 2238, 2676, 1010, 3705, 1010, 6316...                   1  \n",
       "1     [101, 12716, 5912, 2001, 5559, 2010, 7997, 204...                   0  \n",
       "2     [101, 2019, 6041, 2110, 2457, 7979, 5006, 3533...                   1  \n",
       "3     [101, 5125, 4957, 27901, 2099, 2001, 7979, 199...                   0  \n",
       "4     [101, 2006, 2258, 2484, 1010, 4052, 1999, 2811...                   1  \n",
       "...                                                 ...                 ...  \n",
       "2473  [101, 3519, 13266, 1996, 4550, 2250, 2552, 208...                   1  \n",
       "2474  [101, 4707, 5416, 4636, 1010, 4297, 1012, 1010...                   1  \n",
       "2475  [101, 1999, 2826, 1010, 1996, 2212, 2457, 7331...                   0  \n",
       "2476  [101, 2006, 2233, 1022, 1010, 2727, 1010, 2198...                   0  \n",
       "2477  [101, 7253, 2928, 2386, 8617, 1996, 7353, 2000...                   0  \n",
       "\n",
       "[2478 rows x 4 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모든 데이터 bert이용한 토큰화 진행\n",
    "\n",
    "def bert_tokenizer(text, party=False):\n",
    "    if party == True:\n",
    "        tokens = tokenizer.encode_plus(text, add_special_tokens=True, max_length=512, padding='do_not_pad', truncation=True)\n",
    "    else:\n",
    "        tokens = tokenizer.encode_plus(text, add_special_tokens=True, max_length=512, padding='max_length', truncation=True)\n",
    "    return tokens['input_ids']\n",
    "\n",
    "\n",
    "df['facts'] = df['facts'].apply(lambda x: bert_tokenizer(x))\n",
    "df['first_party'] = df['first_party'].apply(lambda x: bert_tokenizer(x, party=True))\n",
    "df['second_party'] = df['second_party'].apply(lambda x: bert_tokenizer(x, party=True))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i, j in zip(df['first_party'], df['second_party']):\n",
    "    if len(i)>max_len:\n",
    "        max_len = len(i)\n",
    "    if len(j)>max_len:\n",
    "        max_len = len(j)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[tensor(101), tensor(6316), tensor(1037), ten...</td>\n",
       "      <td>[[tensor(101), tensor(11458), tensor(1037), te...</td>\n",
       "      <td>[[tensor(101), tensor(2006), tensor(2238), ten...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[tensor(101), tensor(4459), tensor(7343), ten...</td>\n",
       "      <td>[[tensor(101), tensor(5623), tensor(14824), te...</td>\n",
       "      <td>[[tensor(101), tensor(12716), tensor(5912), te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[tensor(101), tensor(5006), tensor(3533), ten...</td>\n",
       "      <td>[[tensor(101), tensor(4116), tensor(12424), te...</td>\n",
       "      <td>[[tensor(101), tensor(2019), tensor(6041), ten...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[tensor(101), tensor(4957), tensor(27901), te...</td>\n",
       "      <td>[[tensor(101), tensor(5232), tensor(102), tens...</td>\n",
       "      <td>[[tensor(101), tensor(5125), tensor(4957), ten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[tensor(101), tensor(2520), tensor(4656), ten...</td>\n",
       "      <td>[[tensor(101), tensor(6041), tensor(102), tens...</td>\n",
       "      <td>[[tensor(101), tensor(2006), tensor(2258), ten...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>[[tensor(101), tensor(9079), tensor(12792), te...</td>\n",
       "      <td>[[tensor(101), tensor(13918), tensor(20145), t...</td>\n",
       "      <td>[[tensor(101), tensor(3519), tensor(13266), te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>[[tensor(101), tensor(26678), tensor(4916), te...</td>\n",
       "      <td>[[tensor(101), tensor(4707), tensor(5416), ten...</td>\n",
       "      <td>[[tensor(101), tensor(4707), tensor(5416), ten...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>[[tensor(101), tensor(25039), tensor(13094), t...</td>\n",
       "      <td>[[tensor(101), tensor(2142), tensor(2163), ten...</td>\n",
       "      <td>[[tensor(101), tensor(1999), tensor(2826), ten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>[[tensor(101), tensor(7521), tensor(1998), ten...</td>\n",
       "      <td>[[tensor(101), tensor(2358), tensor(1012), ten...</td>\n",
       "      <td>[[tensor(101), tensor(2006), tensor(2233), ten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>[[tensor(101), tensor(2928), tensor(2386), ten...</td>\n",
       "      <td>[[tensor(101), tensor(2225), tensor(8584), ten...</td>\n",
       "      <td>[[tensor(101), tensor(7253), tensor(2928), ten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2478 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            first_party  \\\n",
       "0     [[tensor(101), tensor(6316), tensor(1037), ten...   \n",
       "1     [[tensor(101), tensor(4459), tensor(7343), ten...   \n",
       "2     [[tensor(101), tensor(5006), tensor(3533), ten...   \n",
       "3     [[tensor(101), tensor(4957), tensor(27901), te...   \n",
       "4     [[tensor(101), tensor(2520), tensor(4656), ten...   \n",
       "...                                                 ...   \n",
       "2473  [[tensor(101), tensor(9079), tensor(12792), te...   \n",
       "2474  [[tensor(101), tensor(26678), tensor(4916), te...   \n",
       "2475  [[tensor(101), tensor(25039), tensor(13094), t...   \n",
       "2476  [[tensor(101), tensor(7521), tensor(1998), ten...   \n",
       "2477  [[tensor(101), tensor(2928), tensor(2386), ten...   \n",
       "\n",
       "                                           second_party  \\\n",
       "0     [[tensor(101), tensor(11458), tensor(1037), te...   \n",
       "1     [[tensor(101), tensor(5623), tensor(14824), te...   \n",
       "2     [[tensor(101), tensor(4116), tensor(12424), te...   \n",
       "3     [[tensor(101), tensor(5232), tensor(102), tens...   \n",
       "4     [[tensor(101), tensor(6041), tensor(102), tens...   \n",
       "...                                                 ...   \n",
       "2473  [[tensor(101), tensor(13918), tensor(20145), t...   \n",
       "2474  [[tensor(101), tensor(4707), tensor(5416), ten...   \n",
       "2475  [[tensor(101), tensor(2142), tensor(2163), ten...   \n",
       "2476  [[tensor(101), tensor(2358), tensor(1012), ten...   \n",
       "2477  [[tensor(101), tensor(2225), tensor(8584), ten...   \n",
       "\n",
       "                                                  facts  first_party_winner  \n",
       "0     [[tensor(101), tensor(2006), tensor(2238), ten...                   1  \n",
       "1     [[tensor(101), tensor(12716), tensor(5912), te...                   0  \n",
       "2     [[tensor(101), tensor(2019), tensor(6041), ten...                   1  \n",
       "3     [[tensor(101), tensor(5125), tensor(4957), ten...                   0  \n",
       "4     [[tensor(101), tensor(2006), tensor(2258), ten...                   1  \n",
       "...                                                 ...                 ...  \n",
       "2473  [[tensor(101), tensor(3519), tensor(13266), te...                   1  \n",
       "2474  [[tensor(101), tensor(4707), tensor(5416), ten...                   1  \n",
       "2475  [[tensor(101), tensor(1999), tensor(2826), ten...                   0  \n",
       "2476  [[tensor(101), tensor(2006), tensor(2233), ten...                   0  \n",
       "2477  [[tensor(101), tensor(7253), tensor(2928), ten...                   0  \n",
       "\n",
       "[2478 rows x 4 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모든 데이터 bert이용한 토큰화 진행 max len 변경\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df.drop('ID',axis=1, inplace=True)\n",
    "def bert_tokenizer(text, party=False):\n",
    "    if party == True:\n",
    "        tokens = tokenizer.encode_plus(text, add_special_tokens=True, max_length=48, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    else:\n",
    "        tokens = tokenizer.encode_plus(text, add_special_tokens=True, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    return tokens['input_ids']\n",
    "\n",
    "\n",
    "df['facts'] = df['facts'].apply(lambda x: bert_tokenizer(x))\n",
    "df['first_party'] = df['first_party'].apply(lambda x: bert_tokenizer(x, party=True))\n",
    "df['second_party'] = df['second_party'].apply(lambda x: bert_tokenizer(x, party=True))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "class CourtDataset(Dataset):\n",
    "    def __init__(self, dataframe, test=False):\n",
    "        self.df = dataframe\n",
    "        self.test = test\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.test == True:\n",
    "            return torch.cat([self.df['first_party'][idx], self.df['second_party'][idx], self.df['facts'][idx]], dim =1)\n",
    "        return torch.cat([self.df['first_party'][idx], self.df['second_party'][idx], self.df['facts'][idx]], dim =1), self.df['first_party_winner'][idx]\n",
    "    \n",
    "\n",
    "train, val = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "train_dataset = CourtDataset(train)\n",
    "val_dataset = CourtDataset(val)\n",
    "All_dataset = CourtDataset(df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "All_loader = DataLoader(All_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        \n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(608, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 2),)\n",
    "        \n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.squeeze(x, dim=1)\n",
    "        x = x.to(torch.float32)\n",
    "        x = self.sequential(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6730508208274841\n",
      "Epoch: 1, Loss: 0.6414133906364441\n",
      "Epoch: 2, Loss: 0.5733006000518799\n",
      "Epoch: 3, Loss: 0.5191951394081116\n",
      "Epoch: 4, Loss: 0.42605966329574585\n",
      "Epoch: 5, Loss: 0.46058064699172974\n",
      "Epoch: 6, Loss: 0.364758163690567\n",
      "Epoch: 7, Loss: 0.44364404678344727\n",
      "Epoch: 8, Loss: 0.37132352590560913\n",
      "Epoch: 9, Loss: 0.3857962191104889\n",
      "Epoch: 10, Loss: 0.41202864050865173\n",
      "Epoch: 11, Loss: 0.38100987672805786\n",
      "Epoch: 12, Loss: 0.37069976329803467\n",
      "Epoch: 13, Loss: 0.31645047664642334\n",
      "Epoch: 14, Loss: 0.3621540367603302\n",
      "Epoch: 15, Loss: 0.3454536199569702\n",
      "Epoch: 16, Loss: 0.31711921095848083\n",
      "Epoch: 17, Loss: 0.38774794340133667\n",
      "Epoch: 18, Loss: 0.4069177806377411\n",
      "Epoch: 19, Loss: 0.39689919352531433\n",
      "Epoch: 20, Loss: 0.35749542713165283\n",
      "Epoch: 21, Loss: 0.3844413161277771\n",
      "Epoch: 22, Loss: 0.3157564103603363\n",
      "Epoch: 23, Loss: 0.34752920269966125\n",
      "Epoch: 24, Loss: 0.36168769001960754\n",
      "Epoch: 25, Loss: 0.3879421055316925\n",
      "Epoch: 26, Loss: 0.32729870080947876\n",
      "Epoch: 27, Loss: 0.3856944143772125\n",
      "Epoch: 28, Loss: 0.32031774520874023\n",
      "Epoch: 29, Loss: 0.31385183334350586\n",
      "Epoch: 30, Loss: 0.3135700225830078\n",
      "Epoch: 31, Loss: 0.39511415362358093\n",
      "Epoch: 32, Loss: 0.31450343132019043\n",
      "Epoch: 33, Loss: 0.32044467329978943\n",
      "Epoch: 34, Loss: 0.3322402834892273\n",
      "Epoch: 35, Loss: 0.32559335231781006\n",
      "Epoch: 36, Loss: 0.31418803334236145\n",
      "Epoch: 37, Loss: 0.3136478364467621\n",
      "Epoch: 38, Loss: 0.31335678696632385\n",
      "Epoch: 39, Loss: 0.313835471868515\n",
      "Epoch: 40, Loss: 0.31358039379119873\n",
      "Epoch: 41, Loss: 0.3250955641269684\n",
      "Epoch: 42, Loss: 0.40960976481437683\n",
      "Epoch: 43, Loss: 0.3929904103279114\n",
      "Epoch: 44, Loss: 0.3231952488422394\n",
      "Epoch: 45, Loss: 0.3132774531841278\n",
      "Epoch: 46, Loss: 0.31447985768318176\n",
      "Epoch: 47, Loss: 0.3241020739078522\n",
      "Epoch: 48, Loss: 0.34834030270576477\n",
      "Epoch: 49, Loss: 0.3309851586818695\n",
      "Epoch: 50, Loss: 0.3408244252204895\n",
      "Epoch: 51, Loss: 0.3336782157421112\n",
      "Epoch: 52, Loss: 0.34520402550697327\n",
      "Epoch: 53, Loss: 0.3141746520996094\n",
      "Epoch: 54, Loss: 0.3140665888786316\n",
      "Epoch: 55, Loss: 0.3636299669742584\n",
      "Epoch: 56, Loss: 0.31337982416152954\n",
      "Epoch: 57, Loss: 0.31407320499420166\n",
      "Epoch: 58, Loss: 0.3133658170700073\n",
      "Epoch: 59, Loss: 0.31467053294181824\n",
      "Epoch: 60, Loss: 0.3443356156349182\n",
      "Epoch: 61, Loss: 0.31639817357063293\n",
      "Epoch: 62, Loss: 0.3133760988712311\n",
      "Epoch: 63, Loss: 0.34675976634025574\n",
      "Epoch: 64, Loss: 0.3248152434825897\n",
      "Epoch: 65, Loss: 0.3391130268573761\n",
      "Epoch: 66, Loss: 0.31372660398483276\n",
      "Epoch: 67, Loss: 0.3135882318019867\n",
      "Epoch: 68, Loss: 0.3132961690425873\n",
      "Epoch: 69, Loss: 0.34498170018196106\n",
      "Epoch: 70, Loss: 0.3443596661090851\n",
      "Epoch: 71, Loss: 0.3201850652694702\n",
      "Epoch: 72, Loss: 0.3726116418838501\n",
      "Epoch: 73, Loss: 0.31345421075820923\n",
      "Epoch: 74, Loss: 0.3450501263141632\n",
      "Epoch: 75, Loss: 0.344553679227829\n",
      "Epoch: 76, Loss: 0.34680095314979553\n",
      "Epoch: 77, Loss: 0.34440067410469055\n",
      "Epoch: 78, Loss: 0.34465456008911133\n",
      "Epoch: 79, Loss: 0.31616339087486267\n",
      "Epoch: 80, Loss: 0.3306901156902313\n",
      "Epoch: 81, Loss: 0.3441871404647827\n",
      "Epoch: 82, Loss: 0.3190036714076996\n",
      "Epoch: 83, Loss: 0.31367194652557373\n",
      "Epoch: 84, Loss: 0.313294917345047\n",
      "Epoch: 85, Loss: 0.3142687678337097\n",
      "Epoch: 86, Loss: 0.3448226749897003\n",
      "Epoch: 87, Loss: 0.3134951889514923\n",
      "Epoch: 88, Loss: 0.3137967586517334\n",
      "Epoch: 89, Loss: 0.3535572588443756\n",
      "Epoch: 90, Loss: 0.31326350569725037\n",
      "Epoch: 91, Loss: 0.3757628798484802\n",
      "Epoch: 92, Loss: 0.36489054560661316\n",
      "Epoch: 93, Loss: 0.31956371665000916\n",
      "Epoch: 94, Loss: 0.31437957286834717\n",
      "Epoch: 95, Loss: 0.3234744966030121\n",
      "Epoch: 96, Loss: 0.3214145600795746\n",
      "Epoch: 97, Loss: 0.33698001503944397\n",
      "Epoch: 98, Loss: 0.3132841885089874\n",
      "Epoch: 99, Loss: 0.3134840726852417\n"
     ]
    }
   ],
   "source": [
    "def train(model, optim, train_loader, criterion, device, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (x, y) in enumerate(train_loader):\n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            if idx % 100 == 0:\n",
    "                print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n",
    "    return model\n",
    "                \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertClassifier()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "model = train(model, optimizer, All_loader, criterion, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.814516129032256%\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            \n",
    "    print('Accuracy: {}%'.format(100*correct/total))\n",
    "    \n",
    "test(model, val_loader, device)\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[tensor(101), tensor(5096), tensor(19139), te...</td>\n",
       "      <td>[[tensor(101), tensor(2142), tensor(2163), ten...</td>\n",
       "      <td>[[tensor(101), tensor(1996), tensor(3118), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[tensor(101), tensor(23689), tensor(4059), te...</td>\n",
       "      <td>[[tensor(101), tensor(17244), tensor(8586), te...</td>\n",
       "      <td>[[tensor(101), tensor(17244), tensor(8586), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[tensor(101), tensor(2053), tensor(1012), ten...</td>\n",
       "      <td>[[tensor(101), tensor(4419), tensor(2547), ten...</td>\n",
       "      <td>[[tensor(101), tensor(1999), tensor(2526), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[tensor(101), tensor(7157), tensor(23699), te...</td>\n",
       "      <td>[[tensor(101), tensor(2142), tensor(2163), ten...</td>\n",
       "      <td>[[tensor(101), tensor(2076), tensor(2010), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[tensor(101), tensor(16758), tensor(102), ten...</td>\n",
       "      <td>[[tensor(101), tensor(7658), tensor(7811), ten...</td>\n",
       "      <td>[[tensor(101), tensor(1999), tensor(2857), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>[[tensor(101), tensor(21404), tensor(6401), te...</td>\n",
       "      <td>[[tensor(101), tensor(3782), tensor(5096), ten...</td>\n",
       "      <td>[[tensor(101), tensor(2429), tensor(2000), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>[[tensor(101), tensor(21311), tensor(102), ten...</td>\n",
       "      <td>[[tensor(101), tensor(2137), tensor(4744), ten...</td>\n",
       "      <td>[[tensor(101), tensor(2930), tensor(11518), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>[[tensor(101), tensor(8507), tensor(1037), ten...</td>\n",
       "      <td>[[tensor(101), tensor(2520), tensor(1043), ten...</td>\n",
       "      <td>[[tensor(101), tensor(8507), tensor(24713), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>[[tensor(101), tensor(2899), tensor(2110), ten...</td>\n",
       "      <td>[[tensor(101), tensor(5690), tensor(102), tens...</td>\n",
       "      <td>[[tensor(101), tensor(1999), tensor(3285), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>[[tensor(101), tensor(10117), tensor(2358), te...</td>\n",
       "      <td>[[tensor(101), tensor(26026), tensor(7939), te...</td>\n",
       "      <td>[[tensor(101), tensor(2006), tensor(2257), ten...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            first_party  \\\n",
       "0     [[tensor(101), tensor(5096), tensor(19139), te...   \n",
       "1     [[tensor(101), tensor(23689), tensor(4059), te...   \n",
       "2     [[tensor(101), tensor(2053), tensor(1012), ten...   \n",
       "3     [[tensor(101), tensor(7157), tensor(23699), te...   \n",
       "4     [[tensor(101), tensor(16758), tensor(102), ten...   \n",
       "...                                                 ...   \n",
       "1235  [[tensor(101), tensor(21404), tensor(6401), te...   \n",
       "1236  [[tensor(101), tensor(21311), tensor(102), ten...   \n",
       "1237  [[tensor(101), tensor(8507), tensor(1037), ten...   \n",
       "1238  [[tensor(101), tensor(2899), tensor(2110), ten...   \n",
       "1239  [[tensor(101), tensor(10117), tensor(2358), te...   \n",
       "\n",
       "                                           second_party  \\\n",
       "0     [[tensor(101), tensor(2142), tensor(2163), ten...   \n",
       "1     [[tensor(101), tensor(17244), tensor(8586), te...   \n",
       "2     [[tensor(101), tensor(4419), tensor(2547), ten...   \n",
       "3     [[tensor(101), tensor(2142), tensor(2163), ten...   \n",
       "4     [[tensor(101), tensor(7658), tensor(7811), ten...   \n",
       "...                                                 ...   \n",
       "1235  [[tensor(101), tensor(3782), tensor(5096), ten...   \n",
       "1236  [[tensor(101), tensor(2137), tensor(4744), ten...   \n",
       "1237  [[tensor(101), tensor(2520), tensor(1043), ten...   \n",
       "1238  [[tensor(101), tensor(5690), tensor(102), tens...   \n",
       "1239  [[tensor(101), tensor(26026), tensor(7939), te...   \n",
       "\n",
       "                                                  facts  \n",
       "0     [[tensor(101), tensor(1996), tensor(3118), ten...  \n",
       "1     [[tensor(101), tensor(17244), tensor(8586), te...  \n",
       "2     [[tensor(101), tensor(1999), tensor(2526), ten...  \n",
       "3     [[tensor(101), tensor(2076), tensor(2010), ten...  \n",
       "4     [[tensor(101), tensor(1999), tensor(2857), ten...  \n",
       "...                                                 ...  \n",
       "1235  [[tensor(101), tensor(2429), tensor(2000), ten...  \n",
       "1236  [[tensor(101), tensor(2930), tensor(11518), te...  \n",
       "1237  [[tensor(101), tensor(8507), tensor(24713), te...  \n",
       "1238  [[tensor(101), tensor(1999), tensor(3285), ten...  \n",
       "1239  [[tensor(101), tensor(2006), tensor(2257), ten...  \n",
       "\n",
       "[1240 rows x 3 columns]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df.drop('ID',axis=1, inplace=True)\n",
    "\n",
    "test_df['facts'] = test_df['facts'].apply(lambda x: bert_tokenizer(x))\n",
    "test_df['first_party'] = test_df['first_party'].apply(lambda x: bert_tokenizer(x, party=True))\n",
    "test_df['second_party'] = test_df['second_party'].apply(lambda x: bert_tokenizer(x, party=True))\n",
    "\n",
    "test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CourtDataset(test_df, test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "predict_df = pd.DataFrame(columns=['ID', 'first_party_winner'])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, x in enumerate(test_loader):\n",
    "        x = x[0].to(device)\n",
    "        y_pred = model(x)\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        predict_df.loc[idx] = ['TEST_{0:04}'.format(idx), predicted.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>TEST_1235</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>TEST_1236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>TEST_1237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>TEST_1238</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>TEST_1239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  first_party_winner\n",
       "0     TEST_0000                   1\n",
       "1     TEST_0001                   1\n",
       "2     TEST_0002                   1\n",
       "3     TEST_0003                   0\n",
       "4     TEST_0004                   1\n",
       "...         ...                 ...\n",
       "1235  TEST_1235                   1\n",
       "1236  TEST_1236                   1\n",
       "1237  TEST_1237                   0\n",
       "1238  TEST_1238                   1\n",
       "1239  TEST_1239                   1\n",
       "\n",
       "[1240 rows x 2 columns]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "trials = 0\n",
    "while True:\n",
    "    if os.path.exists('submission_{}.csv'.format(trials)):\n",
    "        trials += 1\n",
    "    else:\n",
    "        predict_df.to_csv('submission_{}.csv'.format(trials), index=False)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
